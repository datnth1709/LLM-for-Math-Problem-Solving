class get_config():
  model = 'deepseek-ai/deepseek-math-7b-rl'
  max_new_tokens = 1024
  temperature = 1
  top_p = 0.9
  num_return_sequences = 1
  dataset_train = 'datasets/train.json'
  dataset_test = 'datasets/test.json'
  output_dir = 'experiments'
  per_device_train_batch_size = 1
  gradient_accumulation_steps = 4
  num_train_epochs = 1
  learning_rate = 2e-4
  fp16 = True
  save_total_limit = 3
  logging_steps = 1
  optim = 'paged_adamw_8bit'
  lr_scheduler_type = 'cosine'
  warmup_ratio = 0.05
  lora_r = 4
  lora_alpha = 4
  lora_dropout = 0.05
  push_to_hub = True
  hf_account = 'datnth1709'
  model_hf_name = 'deepseek-math-7b-zaloai2023'